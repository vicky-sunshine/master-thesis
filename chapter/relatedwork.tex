\chapter{Related Work} \label{ch:related_work}

In this chapter begins with a brief review of SDN and provide an overview of how SDN actually works in Section \ref{sec:sdn}, and then we move on describing the detailed work of OpenFlow which is the first SDN standards and the most well-known southbound protocol defined by Open Networking Foundation (ONF) in Section \ref{sec:openflow}.

In Section \ref{sec:nfv}, the NFV is introduced which is a network architecture concept proposed by the European Telecommunications Standards Institute (ETSI) that uses virtualization technologies to virtualize network node functions into building blocks.

Related works of Virtual CPE platform will be described in Section \ref{sec:related_vcpe}. At the first in this section, we will introduce Cloud4NFV and NetFATE, which our HSNL vCPE framework is inspired from. We will also introduce some vCPE products proposed by other telco.

Finally, we will described our previous HSNL vCPE framework in detail in Section \ref{sec:hsnl_vcpe}.





\section{SDN} \label{sec:sdn}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{./fig/sdn_architecture}
\caption{SDN logical architecture.}
\label{fig:sdn_archi}
\end{figure}

SDN has been one of the pillars of innovation in network infrastructures, allowing the decoupling of the control and data planes through an open and standard interface that enables the programmability of the network \cite{sdn-define}.
SDN also centralizes application controls of the whole network so that network managers could directly implement their
ideas.

Figure \ref{fig:sdn_archi} depicts a logical structure view of the SDN architecture.
SDN applications or business applications are programs that built on top of the controller.
These applications performs business logic and send requests to SDN controllers through Northbound APIs.
The most used Northbound API is REST (Representational State Transfer) API.

When receiving the requests from applications, the SDN controller uses the Southbound Interface to communicate with programmable SDN network devices.
The most commonly used is the OpenFlow specification \cite{openflow-spec} defined by Open Networking Foundation (ONF) \cite{onf}.





\section{OpenFlow Protocol} \label{sec:openflow}

Referred to Section \ref{sec:sdn}, SDN architectures generally have three components, SDN applications, SDN controllers, and SDN network devices.
For communication between control plane and data plane, there is defined a southbound interface between Controller and network devices.
In this section, we will introduce the detail of the most commonly used Southbound Interface, OpenFlow protocol.



\subsection{OpenFlow Switch Components}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{./fig/openflow_switch_component}
\caption{Main components of an OpenFlow switch. \cite{openflow-spec}}
\label{fig:openflow_switch_component}
\end{figure}

Fig. \ref{fig:openflow_switch_component} illustrates basic architecture of the OpenFlow-compatible switch. In an OpenFlow Switch, there are multiple flow tables, a group table, a meter table and an OpenFlow channel.

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{./fig/flow_entry}
\caption{Main components of a flow entry in a flow table. \cite{openflow-spec}}
\label{fig:flow_entry}
\end{figure}

The flow table matches incoming packets to a particular flow and specifies the actions that are to be performed on the packets;
There are flows entries in each flow table and each flow table entry contains match fields, priority, instructions, timeouts, and so on as shown in Fig. \ref{fig:flow_entry}.

The group table which is used to arrange multiple flow entries into a group and update the actions based on the group, the meter table can trigger a variety of bandwidth performance-related actions on a flow and the SDN controller communicates to the switch via the OpenFlow channel.



\subsection{Flow Table Pipeline, Matching and Table-miss}

\begin{figure}[!ht]
  % \centering
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{./fig/openflow_pipeline}
    \caption{Packets are matched against multiple tables in the pipeline.}
    \label{fig:openflow_pipeline2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{./fig/openflow_pipeline2}
    \caption{Per-table packet processing.}
    \label{fig:openflow_pipeline3}
  \end{subfigure}
  \caption{Packet flow through the processing pipeline. \cite{openflow-spec}}
  \label{fig:openflow_pipeline}
\end{figure}


\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{./fig/openflow_matching}
\caption{The workflow of packet handling through an OpenFlow switch. \cite{openflow-spec}}
\label{fig:openflow_matching}
\end{figure}

The Packet flow through the processing pipeline is described in Fig. \ref{fig:openflow_pipeline} and the workflow of packet handling through an OpenFlow switch on receipt of a packet is illustrated in Fig. \ref{fig:openflow_matching}.

The processing of each packet always starts at the first flow table.
When being processed by a flow table, the OpenFlow switch will do a table lookup based on the packet type, and various packet header fields, such as source MAC address or destination IP address.
If a flow table entry field has a value of ANY (field omitted), it matches all possible values in the header.
Once the packet is matched highest-priority matching flow entry in the flow table, the counters on the selected flow entry must be updated and the corresponding action must be added to the instruction set.

The packet can execute the instruction set immediately, or execute after finishing the journey in switch.
If actions applied in a previous table using the Apply-Actions changed the packet headers, those changes are reflected in the packet match fields.
A flow entry can direct a packet to next table if there's a GOTO\_TABLE action in instruction set.
The detail of packet matching will be described in next subsection.

Every flow table must support a table-miss flow entry to handle the table missed condition, in which packets unmatched any flow entries in the flow table. The table-miss flow entry is identified by its wildcard patterns and the lowest priority (0). The instructions of the table-miss flow entry maybe the send packets to the controller, drop packets or direct packets to a subsequent table.





\section{NFV} \label{sec:nfv}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{./fig/nfv_and_sdn.png}
\caption{Network Functions Virtualization Relationship with SDN. \cite{nfv-wp}}
\label{fig:nfv_and_sdn}
\end{figure}


Different from the concept of SDN, NFV is used to relocation of network functions from dedicated appliances to generic servers.
Network functions developed by different vendors and provided based on different hardware. This makes them difficult to design, manage and deploy.
To resolve these issues, telecom providers came together in a group called European Telecommunications Standards Institute (ETSI) and the ETSI setups the Industry Specification Group for Network Functions Virtualization (ISG NFV). The working group proposed a new architecture for network virtualization paradigm. The original NFV white paper \cite{nfv-wp}, described the problems that they are facing, along with their proposed solution.

NFV virtualizes network services via software and it decouples the network functions from specialized hardware and aims to replace traditional hardware-based network appliances with virtual appliances.
The benefits of network functions virtualization are: (1) reducing CapEx and OpEX, (2) encouraging more innovation and investment to bring new services quickly at much lower risk., (3) significant reduction in calls to customer support and (4) greater flexibility to scale up, scale down or evolve services


\section{Related vCPE framework} \label{sec:related_vcpe}
\subsection{Cloud4NFV}
\subsection{NetFate}
\subsection{OpenNaaS}
\subsection{Ericsson CPE}
\subsection{Juniper Cloud CPE}
\subsection{Huawei AR1000V virtual router}
\subsection{AT\&T flexware}



\section{HSNL vCPE framework} \label{sec:hsnl_vcpe}
The following part of this paper moves on to describe in greater detail the HSNL vCPE framework. Our proposed vCPE functions, which is implemented by multiple flow table management mechanism, also can be deployed by this framework.


\subsection{Deployment Model}
Unlike a related study that explored the virtualization of network function in PE devices \cite{vcpe-enhance} or used service-chains in data center to achieve vCPE services \cite{ericcson-vcpe}, HSNL introduced a network function service deployment model based on the NetFATE (Network Function at the Edge) approach \cite{netfate}.

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{./fig/hsnl_service_deployment}
\caption{Service deployment model.}
\label{fig:hsnl_service_deployment}
\end{figure}

Fig. \ref{fig:hsnl_service_deployment} illustrates the service deployment model. Each green area is a local network domain of the customer. An SDN switch is presented at the gateway of this domain. The customer can subscribe to our vCPE service through our dashboard. After subscription, the vCPE system creates a new Docker container in which an SDN controller is run. The customer only needs to set up the gateway SDN switch to connect the SDN controller through the OpenFlow protocol; thereafter, the switch executes the service.


\subsection{Architecture of the Main System}
\subsubsection{Reference Architecture - ETSI NFV MANO Model}

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{./fig/etsi_nfv_architecture}
\caption{ETSI MANO Architecture. \cite{etsi-nfv-archi}}
\label{fig:etsi_nfv_architecture}
\end{figure}

HSNL virtual CPE platform \cite{che-wei-master, che-wei-umedia} is inspired by ETSI NFV MANO model and designed under the concept of the ETSI NFV reference architectural framework \cite{etsi-nfv-archi}.

Shown in Fig. \ref{fig:etsi_nfv_architecture}, the right side of the ETSI-NFV architecture are: NFV Orchestrator (NFVO), VNF Manager (VNFM), and Virtualized Infrastructure Manager (VIM). NFVO is responsible for the orchestration and management of NFVI resources and to implement network services on the NFVI. VNFM is responsible for the lifecycle management of VNF instances (instantiation, configuration, update, scale up/down, termination, etc). VIM is responsible for controlling/managing the NFVI resources.

\subsubsection{vCPE Platform NFV Architecture}

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{./fig/hsnl_vcpe_architecture}
\caption{HSNL vCPE MANO Architecture \cite{che-wei-master}.}
\label{fig:hsnl_vcpe_architecture}
\end{figure}

The HSNL virtual CPE architecture (Fig. \ref{fig:hsnl_vcpe_architecture}) expands the scope of ETSI NFV MANO model and we will go more detail in \ref{ssec:hsnl_system_imple}.


\subsection{System Implementation} \label{ssec:hsnl_system_imple}
Turning now to the implementation of HSNL Virtual CPE platform.  The system overview (Fig. \ref{fig:hsnl_vcpe_framework}) includes an infrastructure controller, an infrastructure orchestrator, a cloud database, VNF controllers and a VNF Orchestrator. Each component is introduced in the following subsubsections.

\begin{figure}[!t]
\centering
\includegraphics[width=0.9\textwidth]{./fig/hsnl_vcpe_framework}
\caption{HSNL vCPE framework overview.}
\label{fig:hsnl_vcpe_framework}
\end{figure}

\subsubsection{Infrastructure Controller}
The infrastructure controller comprises a Docker management server that can manage the Docker resources like containers and images and a OpenStack server that can manage the VM resources. The infrastructure controller does not manage customer authentication or maintaining the state of the running service; however, it follows the request from the infrastructure orchestrator to create, delete, start, stop, and inspect containers and VMs.

\subsubsection{Infrastructure Orchestrator}
The infrastructure orchestrator plays a key role in our system. It connects and automates the workflows when our services are deployed. When a customer subscribes to our service, the infrastructure orchestrator first authenticates the customer, calls the infrastructure controller to create a container or a VM for the customer, and then updates the information in the database. The infrastructure orchestrator controls the entire life-cycle of our vCPE services.

\subsubsection{Cloud Database}
The cloud database is used for restoring the meta data of our vCPE services, which include each customer’s credentials, customer’s container/VM settings, and virtual CPE service states. The cloud database employs PostgreSQL, which is an open source, easily customizable and object-relational database system. Only the infrastructure orchestrator has permissions to access the cloud database.

\subsubsection{VNF Controllers}
VNF controllers comprises an SDN controller developed using Ryu framework \cite{ryu} and a remote launcher module. The SDN controller does not have a remote launcher module for remotely executing an SDN controller. We built a light-weight server as a launcher module to resolve this problem. The remote launcher module monitors the SDN controller process ID (PID) and kills the SDN controller PID on demand. Once the infrastructure controller  creates the container or the VM, the remote module will initially runs, waiting for requests from VNF Orchestrator. The virtual CPE functions are achieved by the synergies between the VNF controller and the SDN switch.

\subsubsection{VNF Orchestrator}
The VNF orchestrator is a web application server hosted on Amazon web server, and provides to customers an online dashboard for vCPE services management and configuration. Through the web-based UI provided by the VNF orchestrator, customers can subscribe to the desired service without typing any command through the command line interface. After receiving the subscription message, the VNF orchestrator requests the infrastructure orchestrator to create a new VNF controller, and then sends the virtual CPE configuration to the new VNF controller. Based on configuration demands under different conditions, the network administrator can select any of the listed network functions on the dashboard, such as Firewall, NAT, DHCP, quality of service (QoS) management and our proposed virtual home gateway CPE functions which is implemented by multiple flow table mechanism.
